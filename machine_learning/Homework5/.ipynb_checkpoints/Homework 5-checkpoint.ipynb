{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last homework! Aww yeah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data():\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    return pd.DataFrame(mnist.data / 255.0), pd.Series(mnist.target).astype('int').astype('category')\n",
    "\n",
    "x,y = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50, svd_solver='full')\n",
    "x_pca = pca.fit_transform(x)\n",
    "print(x_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. I used XX components, because the PCA algorithm said that that's how many it took to explain 95% of the variance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca[:, 0], pca[:, 1],\n",
    "            c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The trends that I notice are: XXXXXXXXXXXXXX. This plot is showing us the best possible projection of our data from 784 dimensional space into 2 dimensional space. The axes of this plot are the two components that explain the most variation of all other components. \n",
    "\n",
    "c. You can use PCA to simplify data and reduce its dimensionality to make it easier for other algorithms to handle. You can also use it to compress data, which can be useful for images. It's an unsupervised method, meaning it doesn't reference any labels for its data. Basically, it examines data and looks at the axes or dimensions in the data and calculating how much of the variance in the dataset they each explain. Then, it keeps the top n (n is a hyperparameter) components that explain the most variance. The drawback is that you do lose some of the variance in the dataset, potentially losing some of the trends and patterns in the data.\n",
    "\n",
    "d. For these plots, I use the raw data and the data we compressed with pca earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[1].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_pca[1].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[10000].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_pca[10000].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[30000].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_pca[30000].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They still look pretty good, just a bit fuzzier.\n",
    "\n",
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "rows = np.arange(70000)\n",
    "np.random.shuffle(rows)\n",
    "n_select = 10000\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=50, n_iter=1000, learning_rate=200)\n",
    "tsne_results = tsne.fit_transform(x_pca[rows[:n_select],:]) #Use the already compressed data, as in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "tsne = pd.DataFrame(tsne_results, columns=['comp1', 'comp2'])\n",
    "df_tsne['label'] = y[rows[:n_select]]\n",
    "\n",
    "sns.lmplot(x='comp1', y='comp2', data=df_tsne, hue='label', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is different from the PCA plot in the following ways:\n",
    " - XX\n",
    " - XX\n",
    "\n",
    "It's XXXX robust to changes in perplexity, XXXX robust to changes in number of iterations, and XXXX robust to changes in learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sse = []\n",
    "list_k = list(range(7, 13))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(x)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a, b. I centered my choices for k around 10 because there are 10 digits in the data, making it logical that there will be 10 clusters of data. I evaluated my clustering by using the \"elbow method.\" As you can see from my plot (MAYBE), 10 is a good choice for k.\n",
    "\n",
    "c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of clusters using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Yes.\n",
    "\n",
    "e. Pretty robust. (Ask Nelson about what this means)\n",
    "\n",
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Thanks to https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py for this function!\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=10, linkage='ward')  \n",
    "cluster.fit(x)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off this plot, XX clusters seems reasonable. \n",
    "\n",
    "Here is my dendrogram with different linkages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=10, linkage='complete')  \n",
    "cluster.fit(x)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=10, linkage='average')  \n",
    "cluster.fit(x)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=10, linkage='single')  \n",
    "cluster.fit(x)\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
